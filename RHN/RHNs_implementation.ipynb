{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchtext.data import Field, BPTTIterator\n",
    "from torchtext.datasets import PennTreebank\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use GPU?\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "# Reproducibility\n",
    "torch.manual_seed(42) # Random Number Generator for all devices\n",
    "# When running on the CuDNN backend, two further options must be set.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading ptb.train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\penn-treebank\\ptb.train.txt: 5.10MB [00:02, 2.50MB/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading ptb.valid.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\penn-treebank\\ptb.valid.txt: 400kB [00:00, 1.71MB/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading ptb.test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\penn-treebank\\ptb.test.txt: 450kB [00:00, 2.43MB/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9703\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "spacy_en = spacy.load('en')\n",
    "def tokenize(s):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(s)]\n",
    "\n",
    "# Prepare a field and get the data\n",
    "TEXT = Field(lower=True, tokenize=tokenize)\n",
    "train_data, valid_data, test_data = PennTreebank.splits(TEXT)\n",
    "\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train_data, min_freq=2)\n",
    "print(\"Vocab size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "bptt_len = 35\n",
    "\n",
    "train_loader, valid_loader, test_loader = BPTTIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=batch_size, bptt_len=bptt_len, \n",
    "    device=device, \n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 40]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 35x40 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 35x40 (GPU 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   6,    7,    5,  ...,  538,   98,    8],\n",
       "        [   0,    6,    4,  ...,    2,  204,  633],\n",
       "        [   0,   12,   82,  ...,  110,    2,  555],\n",
       "        ...,\n",
       "        [   3, 1148, 3450,  ...,    8,    5,   36],\n",
       "        [   5,   13, 3446,  ...,    7,    4,  100],\n",
       "        [   4,  151,  660,  ...,    6,   11,  371]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    6,    4,  ...,    2,  204,  633],\n",
       "        [   0,   12,   82,  ...,  110,    2,  555],\n",
       "        [   0,  503,    7,  ...,   10,  542,  130],\n",
       "        ...,\n",
       "        [   5,   13, 3446,  ...,    7,    4,  100],\n",
       "        [   4,  151,  660,  ...,    6,   11,  371],\n",
       "        [   8, 3148,   63,  ...,    2, 4954,  142]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(i.text[1:] - i.target[:-1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9\n",
    "i.text.new(5, 10).bernoulli_(1-p).float().div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.:\n",
    "            return x\n",
    "        shape = (x.size(0), 1, x.size(2))\n",
    "        m = self.dropout_mask(x.data, shape, self.p)\n",
    "        return x * m\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout_mask(x, sz, p):\n",
    "        return x.new(*sz).bernoulli_(1-p).float().div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 first=False,\n",
    "                 couple=False,\n",
    "                 dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "        self.couple = couple\n",
    "        if first:\n",
    "            self.W_H = nn.Linear(in_features, out_features, bias=False)\n",
    "            self.W_T = nn.Linear(in_features, out_features, bias=False)\n",
    "            if not couple:\n",
    "                self.W_C = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.R_H = nn.Linear(in_features, out_features, bias=True)\n",
    "        self.R_T = nn.Linear(in_features, out_features, bias=True)\n",
    "        if not couple:\n",
    "            self.R_C = nn.Linear(in_features, out_features, bias=True)\n",
    "        self.dropout = RNNDropout(dropout_p)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        if self.first:\n",
    "            h = torch.tanh(self.W_H(x) + self.R_H(x))\n",
    "            t = torch.sigmoid(self.W_T(x) + self.R_T(x))\n",
    "            if self.couple:\n",
    "                c = 1 - t\n",
    "            else:\n",
    "                c = torch.sigmoid(self.W_C(x) + self.R_C(x))\n",
    "        else:\n",
    "            h = torch.tanh(self.R_H(x))\n",
    "            t = torch.sigmoid(self.R_T(x))\n",
    "            if self.couple:\n",
    "                c = 1 - t\n",
    "            else:\n",
    "                c = torch.sigmoid(self.R_C(x))\n",
    "        t = self.dropout(t.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        return h * t + s * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentHighway(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_features, \n",
    "                 out_features, \n",
    "                 recurrence_depth=5, \n",
    "                 couple=False, \n",
    "                 dropout_p=0.):\n",
    "        super().__init__()\n",
    "        highways = [\n",
    "            HighwayBlock(in_features, out_features,\n",
    "                         first=True if l == 0 else False,\n",
    "                         couple=couple, dropout_p=dropout_p)\n",
    "            for l in range(recurrence_depth)\n",
    "        ]\n",
    "        self.highways = nn.ModuleList(highways)\n",
    "        self.recurrence_depth = recurrence_depth\n",
    "        self.hidden_dim = out_features\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # Expects input dimensions [seq_len, bsz, input_dim]\n",
    "        outputs = []\n",
    "        for x in input:\n",
    "            for block in self.highways:\n",
    "                hidden = block(x, hidden)\n",
    "            outputs.append(hidden)\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHNLM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 recurrence_depth,\n",
    "                 num_layers=1,\n",
    "                 hidden_dp=0.65,\n",
    "                 recur_dp=0.3,\n",
    "                 tie_weights=True,\n",
    "                 couple=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        rnns = [\n",
    "            RecurrentHighway(\n",
    "                embedding_dim if l == 0 else hidden_dim,\n",
    "                embedding_dim if tie_weights and l != num_layers else hidden_dim,\n",
    "                recurrence_depth=recurrence_depth,\n",
    "                couple=couple,\n",
    "                dropout_p=recur_dp)\n",
    "            for l in range(num_layers)\n",
    "        ]\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        self.fc1 = nn.Linear(embedding_dim if tie_weights else hidden_dim, vocab_size)\n",
    "        self.hidden_dropout = RNNDropout(hidden_dp)\n",
    "        if tie_weights:\n",
    "            self.fc1.weight = self.embedding.weight\n",
    "            \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = [weight.new(bsz, rnn.hidden_dim).zero_() for rnn in self.rnns]\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bptt_len, bsz = x.shape\n",
    "        vocab_size = self.embedding.num_embeddings\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        hiddens = self.init_hidden(bsz)\n",
    "        \n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            out, hidden = rnn(out, hiddens[i])\n",
    "            out = self.hidden_dropout(out)\n",
    "            \n",
    "        out = self.fc1(out.flatten(0, 1))\n",
    "        out = out.view(bptt_len, bsz, vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RHNLM(\n",
    "    vocab_size=len(TEXT.vocab),\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=650,\n",
    "    recurrence_depth=5,\n",
    "    num_layers=2,\n",
    "    recur_dp=0.3,\n",
    "    hidden_dp=0.65,\n",
    "    tie_weights=True,\n",
    "    couple=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = i.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 40)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bptt_len, bsz = x.shape\n",
    "bptt_len, bsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = model.init_hidden(bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 300]), torch.Size([40, 300]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[0].shape, hiddens[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentHighway(\n",
       "  (highways): ModuleList(\n",
       "    (0): HighwayBlock(\n",
       "      (W_H): Linear(in_features=300, out_features=300, bias=False)\n",
       "      (W_T): Linear(in_features=300, out_features=300, bias=False)\n",
       "      (R_H): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (R_T): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (dropout): RNNDropout()\n",
       "    )\n",
       "    (1): HighwayBlock(\n",
       "      (R_H): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (R_T): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (dropout): RNNDropout()\n",
       "    )\n",
       "    (2): HighwayBlock(\n",
       "      (R_H): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (R_T): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (dropout): RNNDropout()\n",
       "    )\n",
       "    (3): HighwayBlock(\n",
       "      (R_H): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (R_T): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (dropout): RNNDropout()\n",
       "    )\n",
       "    (4): HighwayBlock(\n",
       "      (R_H): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (R_T): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (dropout): RNNDropout()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnns[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
