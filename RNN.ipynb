{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_dim = 200\n",
    "emb_dim = 5\n",
    "enc_hid_dim = 3\n",
    "dec_hid_dim = 3\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0],\n",
       "        [ 51,  63],\n",
       "        [133,  67],\n",
       "        [ 10, 118],\n",
       "        [154,  84],\n",
       "        [138,  12],\n",
       "        [ 84, 139],\n",
       "        [183, 159],\n",
       "        [ 52,  58],\n",
       "        [199, 199]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(1, input_dim-1, (seq_len, batch_size))\n",
    "x[0] = torch.LongTensor([0]*2)\n",
    "x[-1] = torch.LongTensor([input_dim-1]*2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(input_dim, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape # seq_len, batch_size, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: default is 1\n",
    "# bias: default is True\n",
    "# batch_first: default is False\n",
    "#     if True, input and output tensor's shape is\n",
    "#         (batch, seq, feature)\n",
    "# dropout: default is 0\n",
    "# bidirectional: default is False\n",
    "rnn = nn.GRU(emb_dim, enc_hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ih, w_hh, b_ih, b_hh = list(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 5]), torch.Size([9, 3]), torch.Size([9]), torch.Size([9]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ih.shape, w_hh.shape, b_ih.shape, b_hh.shape\n",
    "# Since bidirectional is False, num_directions is 1.\n",
    "# Since rnn's hidden unit is GRU, gate_size is 3.\n",
    "# w_ih's shape: (9, 5) == (gate_size * hidden_size, input_size)\n",
    "# w_hh's shape: (9, 3) == (gate_size * hidden_size, hidden_size)\n",
    "# b_ih's shape: (9) == (gate_size * hidden_size)\n",
    "# b_hh's shape: (9) == (gate_size * hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_directions = 1\n",
    "max_batch_size = embedded.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = torch.zeros(num_layers*num_directions,\n",
    "                 max_batch_size, enc_hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 5])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_t = torch.sigmoid(\n",
    "    embedded.matmul(w_ih[:3,:].T.contiguous()) + b_ih[:3] + \n",
    "    hx.matmul(w_hh[:3,:].T.contiguous()) + b_hh[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t = torch.sigmoid(\n",
    "    embedded.matmul(w_ih[3:6,:].T.contiguous()) + b_ih[3:6] + \n",
    "    hx.matmul(w_hh[3:6,:].T.contiguous()) + b_hh[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = torch.tanh(\n",
    "    embedded.matmul(w_ih[6:,:].T.contiguous()) + b_ih[6:] + \n",
    "    r_t * (hx.matmul(w_hh[6:,:].T.contiguous()) + b_hh[6:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = (1 - z_t) * n_t + z_t * hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hiddens = rnn(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2, 3]), torch.Size([1, 2, 3]))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1079, -0.1235,  0.0699],\n",
       "         [ 0.1079, -0.1235,  0.0699]],\n",
       "\n",
       "        [[ 0.1702,  0.8597,  0.5208],\n",
       "         [-0.1957,  0.5138,  0.0871]],\n",
       "\n",
       "        [[-0.3517,  0.5044,  0.0150],\n",
       "         [ 0.1909, -0.0404,  0.4272]],\n",
       "\n",
       "        [[ 0.4123,  0.2468,  0.2104],\n",
       "         [-0.1673,  0.6032, -0.0051]],\n",
       "\n",
       "        [[ 0.1030, -0.2435,  0.0642],\n",
       "         [ 0.0549,  0.6815,  0.3285]],\n",
       "\n",
       "        [[-0.2560,  0.4002, -0.1629],\n",
       "         [ 0.0359,  0.3880,  0.2751]],\n",
       "\n",
       "        [[ 0.0549,  0.6815,  0.3285],\n",
       "         [-0.3142,  0.6174, -0.1427]],\n",
       "\n",
       "        [[ 0.3864,  0.1922,  0.2020],\n",
       "         [-0.3375,  0.1038,  0.1796]],\n",
       "\n",
       "        [[ 0.1717, -0.2850, -0.2530],\n",
       "         [ 0.5966,  0.6466, -0.0836]],\n",
       "\n",
       "        [[-0.2410,  0.7671, -0.0040],\n",
       "         [-0.2410,  0.7671, -0.0040]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1882,  0.7398, -0.0412],\n",
       "         [-0.2194,  0.8933, -0.1362]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171,  0.0963,  0.5388],\n",
       "        [ 0.1237, -0.3646, -0.0688],\n",
       "        [ 0.5130,  0.2453,  0.3575],\n",
       "        [-0.4128, -0.5138,  0.5604],\n",
       "        [ 0.3955,  0.5427,  0.3718]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ih[:3,:].permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171,  0.0963,  0.5388],\n",
       "        [ 0.1237, -0.3646, -0.0688],\n",
       "        [ 0.5130,  0.2453,  0.3575],\n",
       "        [-0.4128, -0.5138,  0.5604],\n",
       "        [ 0.3955,  0.5427,  0.3718]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ih[:3,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171,  0.1237,  0.5130, -0.4128,  0.3955],\n",
       "        [ 0.0963, -0.3646,  0.2453, -0.5138,  0.5427],\n",
       "        [ 0.5388, -0.0688,  0.3575,  0.5604,  0.3718]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ih[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
