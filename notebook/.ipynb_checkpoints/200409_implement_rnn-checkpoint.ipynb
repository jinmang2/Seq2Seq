{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Callable, Union\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
    "RnnStateStorage = Tuple[torch.Tensor, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN     = 15\n",
    "BATCH_SIZE  = 5\n",
    "INPUT_DIM   = 30\n",
    "OUTPUT_DIM  = 37\n",
    "HID_DIM     = 256\n",
    "ENC_DROPOUT = DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 17, 16,  3, 22, 21, 27, 18, 20, 29,  1,  1,  1,  1,  1],\n",
      "        [ 0,  3,  6, 20, 27,  8, 18, 13, 27, 13, 25, 12, 11, 29,  1],\n",
      "        [ 0, 13, 20,  9, 22, 24, 29,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 23,  9, 10, 26, 27, 23,  6,  6,  8, 21,  3, 29,  1,  1],\n",
      "        [ 0, 13, 16,  4, 15, 27,  2, 25, 21, 25, 12,  3, 25, 25, 29]]) torch.Size([5, 15])\n",
      "\n",
      "tensor([[ 0, 31,  6,  3,  8, 16, 19,  2, 21, 26, 31, 36,  1,  1,  1],\n",
      "        [ 0, 33, 21,  5, 30, 33, 36,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0,  3,  8, 12, 27, 15,  6, 34, 29, 36,  1,  1,  1,  1,  1],\n",
      "        [ 0, 31,  7, 12,  3, 15, 34, 28, 28, 27, 36,  1,  1,  1,  1],\n",
      "        [ 0, 10, 25, 30, 30, 21, 10, 13, 30, 18, 33,  7, 23, 17, 36]]) torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "SRC_PAD_IDX = TRG_PAD_IDX = 1\n",
    "MIN_WORDS   = 5\n",
    "\n",
    "src_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "trg_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "if SEQ_LEN - 1 not in src_seq_length:\n",
    "    src_seq_length[-1] = SEQ_LEN - 2\n",
    "if SEQ_LEN - 1 not in trg_seq_length:\n",
    "    trg_seq_length[-1] = SEQ_LEN - 2\n",
    "\n",
    "x = torch.randint(0+2, INPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "x[:, 0] = 0\n",
    "for i, ind in enumerate(src_seq_length):\n",
    "    x[i, ind+1 ] = INPUT_DIM - 1\n",
    "    x[i, ind+2:] = SRC_PAD_IDX\n",
    "\n",
    "y = torch.randint(0+2, OUTPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "y[:, 0] = 0\n",
    "for i, ind in enumerate(trg_seq_length):\n",
    "    y[i, ind+1 ] = OUTPUT_DIM - 1\n",
    "    y[i, ind+2:] = TRG_PAD_IDX\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 14,  7, 13, 15])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x != 1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RNNEncoderBase(nn.Module):\n",
    "    \n",
    "    def __init__(self, stateful: bool=False) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.stateful = stateful\n",
    "        self._states: Optional[RnnStateStorage] = None\n",
    "    \n",
    "    def sort_and_run_forward(self,\n",
    "                             module: Callable[\n",
    "                                 [PackedSequence, Optional[RnnState]],\n",
    "                                 Tuple[Union[PackedSequence, torch.Tensor], RnnState]\n",
    "                             ],\n",
    "                             inputs: torch.Tensor,\n",
    "                             pad_token: int,\n",
    "                             hidden_state: Optional[RnnState] = None):\n",
    "        \"\"\"input을 정렬한 후에 forward module을 돌려 output과 final hidden state를 반환\"\"\"\n",
    "        # Sorting\n",
    "        batch_size = mask.size(0) # batch_first=True\n",
    "        max_sequence_length = inputs.size(1)\n",
    "        sequence_length = (inputs != pad_token).sum(dim=-1)\n",
    "        sorted_sequence_length, permutation_index = sequence_length.sort(0, descending=True)\n",
    "        sorted_inputs = inputs.index_select(0, permutation_index)\n",
    "        _, restoration_indices = permutation_index.sort(0, descending=False)\n",
    "        \n",
    "        # Packing\n",
    "        packed_sequence_input = pack_padded_sequence(sorted_inputs,\n",
    "                                                     sorted_sequence_length.data.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        \n",
    "        # Prepare the initial states\n",
    "        if not self.stateful:\n",
    "            if hidden_state == None:\n",
    "                initial_states = hidden_state # Set None\n",
    "            elif isinstance(hidden_states, tuple):\n",
    "                initial_states = [state.index_select(1, sorting_indices)\n",
    "                                  for state in hidden_state]\n",
    "            else:\n",
    "                initial_states = self._get_initial_states(\n",
    "                    batch_size, max_sequence_length, sorting_indices)\n",
    "        else:\n",
    "            initial_states = self._get_initial_states(\n",
    "                batch_size, max_sequence_length, sorting_indices)\n",
    "            \n",
    "        # Actually call the module on the sorted PackedSequence\n",
    "        module_output, final_states = module(packed_sequence_input, initial_states)\n",
    "        \n",
    "        return module_output, final_states, restoration_indices\n",
    "\n",
    "    def _get_initial_states(self,\n",
    "                            batch_size: int,\n",
    "                            max_sequence_length: int,\n",
    "                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n",
    "        \"\"\"\n",
    "        (a) RNN의 초기 상태를 반환\n",
    "        (b) batch의 새로운 요소에 대한 초기 상태를 추가하기 위해\n",
    "           메서드를 호출, 상태를 변경(mutate)하여 바뀐 batch_size를 handling\n",
    "        (c) batch의 각 요소(sentence)의 sequence 길이로 state를 정렬\n",
    "        (d) pad 작업 수행 후 행을 제거\n",
    "        (e) 이전에 호출됐을 때보다 batch_size가 크면 상태를 변경(mutate)\n",
    "            - (b)의 특별 case.\n",
    "\n",
    "        이 메서드는 \n",
    "            (1) 처음 호출되어 아무 상태가 없는 경우 \n",
    "            (2) RNN이 heterogeneous state를 가질 때\n",
    "        의 경우를 처리해야 하기 때문에 return값이 복잡함\n",
    "\n",
    "        (1) module이 처음 호출됬을 때 ``module``의 타입이 무엇이든 ``None`` 반환\n",
    "        (2) Otherwise,\n",
    "            - LSTM의 경우 tuple of ``torch.Tensor``\n",
    "              shape: ``(num_layers, max_seq_len, state_size)``\n",
    "                 and ``(num_layers, max_seq_len, memory_size)``\n",
    "            - GRU의 경우  single ``torch.Tensor``\n",
    "              shape: ``(num_layers, max_seq_len, state_size)``\n",
    "        \"\"\"\n",
    "        # (1)의 경우 처리\n",
    "        if self._states is None:\n",
    "            return None\n",
    "        \n",
    "        # (2)의 경우 처리\n",
    "        if batch_size > self._states[0].size(1):\n",
    "            # (e)의 경우 처리\n",
    "            num_states_to_concat = batch_size - self._states[0].size(1)\n",
    "            resized_states = []\n",
    "            # state의 shape은 (num_layers, batch_size, hidden_size)\n",
    "            for state in self._states:\n",
    "                zeros = state.data.new(state.size(0),\n",
    "                                       num_states_to_concat,\n",
    "                                       state.size(2)).fill_(0)\n",
    "                zeros = Variable(zeros) # torch 1.4.0에선 Variable과 Tensor가 같음\n",
    "                resized_states.append(torch.cat([state, zeros], dim=1))\n",
    "            self._states = tuple(resized_states)\n",
    "            correctly_shaped_states = self._states\n",
    "        elif batch_size < self._states[0].size(1):\n",
    "            # (b)의 경우 처리\n",
    "            correctly_shaped_states = tuple(state[:, :batch_size, :] for state in self._states)\n",
    "        else:\n",
    "            correctly_shaped_states = self._states\n",
    "        \n",
    "        if len(self._states) == 1:\n",
    "            # (2)에서 GRU에 해당\n",
    "            correctly_shaped_states = correctly_shaped_states[0] # unpack from tuple\n",
    "            # (c)처리\n",
    "            sorted_state = correctly_shaped_states.index_select(1, sorting_indices)\n",
    "            return sorted_state\n",
    "        else:\n",
    "            # (2)에서 LSTM에 해당\n",
    "            sorted_states = [states.index_select(1, sorting_indices)\n",
    "                             for state in correctly_shaped_states]\n",
    "            return tuple(sorted_states)\n",
    "        \n",
    "    def _update_states(self,\n",
    "                       final_states: RnnStateStorage,\n",
    "                       restoration_indices: torch.LongTensor) -> None:\n",
    "        # TODO(Mark): seems weird to sort here, but append zeros in the subclasses\n",
    "        # which way around is best?\n",
    "        new_unsorted_states = [state.index_select(1, restoration_indices)\n",
    "                               for state in final_states]\n",
    "        \n",
    "        if self._states is None:\n",
    "            self._states = tuple(\n",
    "                [Variable(state.data) \n",
    "                 for state in new_unsorted_states]\n",
    "            )\n",
    "        else:\n",
    "            # 어떤 상태가 RNN 계산에 사용될 지 나타내기 위해\n",
    "            # (new_batch_size,) 크기의 mask를 생성\n",
    "            current_state_batch_size = self._states[0].size(1)\n",
    "            new_state_batch_size = final_states[0].size(1)\n",
    "            # 사용하지 않은 state에 대한 mask (1, new_batch_size, 1)\n",
    "            used_new_rows_mask = [(state[0, :, :].sum(-1) != 0.0).float().view(\n",
    "                                  1, new_state_batch_size, 1)\n",
    "                                  for state in new_unsorted_states]\n",
    "            new_states = []\n",
    "            if current_state_batch_size > new_state_batch_size:\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # all zero인 row(사용하지 않은 state)만 살림\n",
    "                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n",
    "                    # 사용하여 업데이트할 상태 + 사용하지 않은 상태로 기존 상태를 채움\n",
    "                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n",
    "                    # 계산 그래프 분리\n",
    "                    new_states.append(Variable(old_state.data))\n",
    "            else:\n",
    "                for old_state, new_state, used_mask in zip(self._states,\n",
    "                                                           new_unsorted_states,\n",
    "                                                           used_new_rows_mask):\n",
    "                    # all zero인 row(사용하지 않은 state)만 살림\n",
    "                    masked_old_state = old_state * (1 - used_mask)\n",
    "                    # 사용하지 않았던 상태의 값들을 이번엔 new_state에 기록\n",
    "                    new_state += masked_old_state\n",
    "                    # 계산 그래프 분리\n",
    "                    new_states.append(Variable(new_state))\n",
    "            # 왜 current_state_batch_size < new_state_batch_size인 경우를 고려하지 않는가?\n",
    "            # `_get_initial_state` 메서드에서 이미 처리\n",
    "            self._states = tuple(new_states)\n",
    "        \n",
    "    def reset_states(self):\n",
    "        self._states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
