{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None, dk=64):\n",
    "        # |Q| = (batch_size, m, hidden_size)\n",
    "        # |K| = |V| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        w = torch.bmm(Q, K.transpose(1, 2))\n",
    "        # |w| = (batch_size, m, n)\n",
    "        if mask is not None:\n",
    "            assert w.size() == mask.size()\n",
    "            w.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "        w = self.softmax(w / (dk**.5))\n",
    "        c = torch.bmm(w, V)\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return c\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_splits):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "        # Note that we don't have to declare each linear layer, separately.\n",
    "        self.Q_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.attn = Attention()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # |Q| = (batch_size, m, hidden_size)\n",
    "        # |K| = |V| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        QWs = self.Q_linear(Q).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        KWs = self.K_linear(K).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        VWs = self.V_linear(V).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        # |QW_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        # |KW_i| = |VW_i| = (batch_size, n, hidden_size / n_splits)\n",
    "\n",
    "        # By concatenating splited linear transformed results,\n",
    "        # we can remove sequential operations,\n",
    "        # like mini-batch parallel operations.\n",
    "        QWs = torch.cat(QWs, dim=0)\n",
    "        KWs = torch.cat(KWs, dim=0)\n",
    "        VWs = torch.cat(VWs, dim=0)\n",
    "        # |QWs| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "        # |KWs| = |VWs| = (batch_size * n_splits, n, hidden_size / n_splits)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = torch.cat([mask for _ in range(self.n_splits)], dim=0)\n",
    "            # |mask| = (batch_size * n_splits, m, n)\n",
    "\n",
    "        c = self.attn(\n",
    "            QWs, KWs, VWs,\n",
    "            mask=mask,\n",
    "            dk=self.hidden_size // self.n_splits,\n",
    "        )\n",
    "        # |c| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "\n",
    "        # We need to restore temporal mini-batchfied multi-head attention results.\n",
    "        c = c.split(Q.size(0), dim=0)\n",
    "        # |c_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        c = self.linear(torch.cat(c, dim=-1))\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_splits,\n",
    "                 dropout_p=.1, use_leaky_relu=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHead(hidden_size, n_splits)\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LeakyReLU() if use_leaky_relu else nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # |x| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, n, n)\n",
    "        print('\\n\\n\\n\\n\\n\\n')\n",
    "        print(x.mean())\n",
    "        print(mask.shape)\n",
    "        z = self.attn_norm(x + self.attn_dropout(self.attn(Q=x,\n",
    "                                                           K=x,\n",
    "                                                           V=x,\n",
    "                                                           mask=mask)))\n",
    "        z = self.fc_norm(z + self.fc_dropout(self.fc(z)))\n",
    "        # |z| = (batch_size, n, hidden_size)\n",
    "        print(z.mean())\n",
    "        return z, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Sequential):\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # nn.Sequential class does not provide multiple input arguments and returns.\n",
    "        # Thus, we need to define new class to solve this issue.\n",
    "        # Note that each block has same function interface.\n",
    "\n",
    "        for module in self._modules.values():\n",
    "            x = module(*x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MySequential(\n",
    "            *[EncoderBlock(\n",
    "                512,\n",
    "                8,\n",
    "                0.1,\n",
    "                False,\n",
    "              ) for _ in range(8)],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[    2, 10549,  3900, 45910,  3680, 45908,  2282,    16,     3,     1,\n",
    "             1,     1],\n",
    "        [    2,  2977,  5923,   956, 88842,    39,  2539,     3,     1,     1,\n",
    "             1,     1],\n",
    "        [    2,  1230,  1231,  1232,  1233,   323,  1234,  1235,  1236,   110,\n",
    "             3,     1],\n",
    "        [    2,  6726, 22523,  2457,   804, 61496,   105, 78511,  1664, 11442,\n",
    "         71906,     3],\n",
    "        [    2,  3085,   394,  2615,   133, 26667,     3,     1,     1,     1,\n",
    "             1,     1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 10549,  3900, 45910,  3680, 45908,  2282,    16,     3,     1,\n",
       "             1,     1],\n",
       "        [    2,  2977,  5923,   956, 88842,    39,  2539,     3,     1,     1,\n",
       "             1,     1],\n",
       "        [    2,  1230,  1231,  1232,  1233,   323,  1234,  1235,  1236,   110,\n",
       "             3,     1],\n",
       "        [    2,  6726, 22523,  2457,   804, 61496,   105, 78511,  1664, 11442,\n",
       "         71906,     3],\n",
       "        [    2,  3085,   394,  2615,   133, 26667,     3,     1,     1,     1,\n",
       "             1,     1]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_x = nn.Embedding(x.max()+5, 512)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7817,  0.3814,  1.1691,  ...,  0.3171, -1.5591,  0.2325],\n",
       "         [-0.1527,  0.2612,  0.7388,  ..., -0.1990,  0.8230, -0.0373],\n",
       "         [ 0.6160, -0.4583,  0.1465,  ...,  0.6104,  0.3407,  1.5700],\n",
       "         ...,\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559]],\n",
       "\n",
       "        [[ 0.7817,  0.3814,  1.1691,  ...,  0.3171, -1.5591,  0.2325],\n",
       "         [-0.3984,  0.0500,  0.4003,  ...,  0.8389,  0.6758,  1.1031],\n",
       "         [ 0.4136,  0.6204, -0.7397,  ..., -0.1243,  0.4232,  0.7346],\n",
       "         ...,\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559]],\n",
       "\n",
       "        [[ 0.7817,  0.3814,  1.1691,  ...,  0.3171, -1.5591,  0.2325],\n",
       "         [ 0.9963,  0.2772, -0.6779,  ..., -0.0947,  1.2092,  0.0057],\n",
       "         [-0.9627, -0.3183,  0.6749,  ...,  0.2384,  1.5698, -0.0849],\n",
       "         ...,\n",
       "         [ 0.0250,  0.1714,  2.0796,  ...,  0.5254, -0.4440, -1.5918],\n",
       "         [-1.1475, -0.9625,  0.3301,  ..., -1.5649, -0.1647, -1.3087],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559]],\n",
       "\n",
       "        [[ 0.7817,  0.3814,  1.1691,  ...,  0.3171, -1.5591,  0.2325],\n",
       "         [-0.2411, -0.1488,  1.1389,  ..., -0.3768, -0.9011, -0.8647],\n",
       "         [-0.0175, -0.3372,  1.0127,  ...,  1.1076,  0.5821, -0.4106],\n",
       "         ...,\n",
       "         [-0.6336, -1.4988, -0.1679,  ...,  1.2361, -1.2317, -0.0092],\n",
       "         [-0.6847,  0.8532,  0.6848,  ...,  1.3983, -0.9743,  0.2998],\n",
       "         [-1.1475, -0.9625,  0.3301,  ..., -1.5649, -0.1647, -1.3087]],\n",
       "\n",
       "        [[ 0.7817,  0.3814,  1.1691,  ...,  0.3171, -1.5591,  0.2325],\n",
       "         [ 1.0709, -2.3666,  1.4588,  ..., -0.9400,  0.0428, -0.4984],\n",
       "         [-0.1173, -1.1536,  0.2123,  ...,  0.6032, -0.1894, -0.5297],\n",
       "         ...,\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559],\n",
       "         [ 0.3604, -0.9384, -0.6313,  ..., -0.1238, -1.0589, -1.2559]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = x == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_enc = mask.unsqueeze(1).expand(mask.size(0), x.size(1), mask.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 12])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = embedded_x.new_zeros(embedded_x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(0, enc.size(0)).float().unsqueeze(-1)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (10000. ** (torch.arange(0, 512//2).div(512))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc[:, 0::2] = torch.sin(pos / dim)\n",
    "enc[:, 1::2] = torch.cos(pos / dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_x += enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.Dropout(0.2)(embedded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9771,  1.7267,  1.4614,  ...,  1.6464, -1.9489,  1.5406],\n",
       "         [ 0.8610,  1.0018,  1.9753,  ...,  0.0000,  2.0805,  0.6288],\n",
       "         [ 1.9066, -0.0000,  1.3198,  ...,  0.2428,  1.5625,  1.4423],\n",
       "         ...,\n",
       "         [ 0.9657, -0.0000, -0.2739,  ..., -1.2937, -0.8085, -2.7088],\n",
       "         [-0.2295, -2.2218, -1.4691,  ..., -1.2036, -2.0037, -2.6188],\n",
       "         [-0.7994, -1.1675, -2.0391,  ..., -0.1492, -2.5737, -1.5644]],\n",
       "\n",
       "        [[ 0.9771,  1.7267,  0.0000,  ...,  1.6464, -1.9489,  1.5406],\n",
       "         [ 0.5539,  0.0000,  0.0000,  ...,  1.7240,  1.8966,  2.0543],\n",
       "         [ 1.6536,  0.0000,  0.2120,  ..., -0.6755,  1.6656,  0.3980],\n",
       "         ...,\n",
       "         [ 0.9657, -2.3119, -0.2739,  ..., -1.2937, -0.0000, -2.7088],\n",
       "         [-0.0000, -2.2218, -0.0000,  ..., -0.0000, -2.0037, -2.6188],\n",
       "         [-0.7994, -1.1675, -2.0391,  ..., -0.1492, -2.5737, -1.5644]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  1.4614,  ...,  1.6464, -0.0000,  1.5406],\n",
       "         [ 2.2973,  1.0219,  0.2045,  ...,  0.5570,  2.5634,  0.0000],\n",
       "         [-0.0667, -0.9181,  0.0000,  ..., -0.2222,  3.0988, -0.6264],\n",
       "         ...,\n",
       "         [ 0.5464, -0.9247,  3.1147,  ..., -0.4822, -0.0399, -3.1287],\n",
       "         [-0.0000, -0.0000, -0.2674,  ..., -0.0000, -0.8859, -0.0000],\n",
       "         [-0.7994, -0.0000, -2.0391,  ..., -0.1492, -2.5737, -1.5644]],\n",
       "\n",
       "        [[ 0.9771,  1.7267,  1.4614,  ...,  0.0000, -1.9489,  1.5406],\n",
       "         [ 0.7505,  0.0000,  2.4755,  ...,  0.0000, -0.0745, -0.0000],\n",
       "         [ 1.1148, -0.9417,  0.0000,  ...,  0.8643,  0.0000, -1.0334],\n",
       "         ...,\n",
       "         [-0.2768, -0.0000,  0.3053,  ...,  0.4062, -0.0000, -1.1504],\n",
       "         [-1.5359,  0.0000,  0.1760,  ...,  0.0000, -1.8979, -0.6741],\n",
       "         [-0.0000, -1.1976, -0.8374,  ..., -0.0000, -1.4559, -1.6303]],\n",
       "\n",
       "        [[ 0.9771,  1.7267,  1.4614,  ...,  1.6464, -1.9489,  1.5406],\n",
       "         [ 2.3905, -2.2829,  2.8753,  ..., -0.4996,  1.1053,  0.0523],\n",
       "         [ 0.9900, -1.9622,  1.4020,  ...,  0.0000,  0.8998, -1.1823],\n",
       "         ...,\n",
       "         [ 0.9657, -2.3119, -0.2739,  ..., -1.2937, -0.8085, -2.7088],\n",
       "         [-0.0000, -2.2218, -1.4691,  ..., -0.0000, -2.0037, -2.6188],\n",
       "         [-0.0000, -1.1675, -2.0391,  ..., -0.1492, -2.5737, -1.5644]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(-0.0028, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(-1.1859e-08, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(-1.1859e-08, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(-2.9802e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(-2.9802e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(-7.9473e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(-7.9473e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(-9.1890e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(-9.1890e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(4.2220e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(4.2220e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(3.4769e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(3.4769e-09, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(9.9341e-10, grad_fn=<MeanBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor(9.9341e-10, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 12, 12])\n",
      "tensor(6.9539e-09, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.1711e-01,  2.6203e-02,  1.9705e+00,  ...,  1.3971e+00,\n",
       "           -1.9742e+00,  8.4321e-01],\n",
       "          [ 7.6448e-01, -4.3853e-01,  1.0880e+00,  ..., -6.9765e-03,\n",
       "            8.7684e-01, -1.8241e-02],\n",
       "          [ 1.5311e+00, -1.5038e-01,  9.8502e-01,  ...,  3.0358e-01,\n",
       "            2.5034e-01,  1.8279e+00],\n",
       "          ...,\n",
       "          [ 7.8941e-01, -9.3172e-02,  6.8404e-01,  ...,  2.3345e-01,\n",
       "           -7.9399e-01, -3.6145e-01],\n",
       "          [-2.1299e-01, -1.7069e+00,  2.1603e-02,  ...,  3.3092e-01,\n",
       "           -9.4661e-01, -2.4939e-02],\n",
       "          [-2.0125e-01, -1.0280e+00, -4.2630e-01,  ...,  1.4328e+00,\n",
       "           -9.7178e-01,  6.7541e-01]],\n",
       " \n",
       "         [[ 6.5046e-01,  8.5465e-02,  2.2483e-01,  ...,  5.7862e-01,\n",
       "           -1.5922e+00, -4.8244e-01],\n",
       "          [-2.8916e-01, -1.5411e+00,  4.4682e-01,  ...,  1.0592e+00,\n",
       "            5.5628e-01,  7.6843e-01],\n",
       "          [ 2.2324e+00, -9.1635e-01,  8.9765e-01,  ..., -1.6690e-01,\n",
       "            5.0763e-01, -7.0927e-02],\n",
       "          ...,\n",
       "          [ 2.0050e+00, -2.0271e+00,  1.6321e-01,  ...,  7.5786e-01,\n",
       "            7.4415e-03, -9.2557e-01],\n",
       "          [ 8.8540e-01, -1.5297e+00,  4.2961e-01,  ...,  1.5570e+00,\n",
       "           -5.4976e-01, -1.0537e+00],\n",
       "          [ 3.0678e-01, -1.2562e+00, -5.4906e-01,  ...,  6.6681e-01,\n",
       "           -3.7746e-01, -7.1574e-01]],\n",
       " \n",
       "         [[ 7.8831e-01, -7.8581e-02,  6.3839e-01,  ...,  1.7186e+00,\n",
       "           -9.5881e-01,  1.9258e-01],\n",
       "          [ 2.4761e+00,  3.2121e-01,  2.2745e-01,  ..., -6.5992e-01,\n",
       "            2.4815e-01, -1.1707e+00],\n",
       "          [ 1.2282e+00, -6.0584e-01,  2.2168e-01,  ...,  2.8953e-01,\n",
       "            1.8666e+00,  3.3853e-02],\n",
       "          ...,\n",
       "          [ 1.2120e+00, -1.6714e+00,  1.2716e+00,  ...,  1.2863e+00,\n",
       "           -4.4389e-01, -2.1249e+00],\n",
       "          [ 9.2021e-01, -2.9643e-01,  1.4231e-03,  ...,  1.2206e+00,\n",
       "           -1.0491e+00,  2.8775e-01],\n",
       "          [ 6.4717e-02, -2.9779e-01, -1.7897e+00,  ...,  9.1078e-01,\n",
       "           -1.2949e+00, -4.3109e-01]],\n",
       " \n",
       "         [[ 2.0459e-01,  1.0202e-01,  1.5703e+00,  ...,  2.8361e-01,\n",
       "           -1.8226e+00,  3.7623e-01],\n",
       "          [ 1.3098e+00, -1.4848e+00,  8.1478e-01,  ...,  1.0907e-02,\n",
       "           -8.2355e-01, -1.0324e+00],\n",
       "          [ 1.1245e+00, -1.6196e+00,  6.0087e-02,  ...,  3.1036e-01,\n",
       "            4.7734e-02, -5.6052e-01],\n",
       "          ...,\n",
       "          [ 3.0668e-01, -1.0576e+00,  1.1553e+00,  ..., -1.8255e-01,\n",
       "            2.0464e-01,  3.0871e-01],\n",
       "          [-3.5726e-01, -8.2966e-04,  3.3704e-01,  ...,  1.3568e+00,\n",
       "           -8.3649e-01,  3.5158e-01],\n",
       "          [ 9.7580e-01, -1.1521e+00, -1.4202e-01,  ...,  5.7675e-01,\n",
       "           -2.5142e-01, -5.4043e-01]],\n",
       " \n",
       "         [[ 1.2248e+00,  1.2888e-01,  8.2932e-01,  ...,  4.5917e-01,\n",
       "           -1.4549e+00,  7.7854e-01],\n",
       "          [ 1.0526e+00, -1.8306e+00,  1.8622e+00,  ..., -1.7177e+00,\n",
       "            4.1824e-01, -5.1560e-01],\n",
       "          [ 7.8066e-01, -1.3519e+00,  5.0743e-01,  ..., -5.2592e-01,\n",
       "            1.5407e-01,  4.7433e-01],\n",
       "          ...,\n",
       "          [ 1.4224e+00, -2.2757e+00, -6.4698e-01,  ..., -1.3987e-01,\n",
       "            3.4325e-01, -1.2371e-01],\n",
       "          [ 5.0090e-01, -1.7809e+00, -1.9276e-01,  ...,  1.6303e+00,\n",
       "           -3.9840e-01,  3.3677e-01],\n",
       "          [-1.9876e-02, -7.9298e-01, -1.3632e+00,  ...,  9.7598e-01,\n",
       "           -1.2324e-01,  4.3374e-01]]], grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False, False,  True,\n",
       "            True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False, False,  True,  True,\n",
       "            True,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False,  True]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False],\n",
       "          [False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False]],\n",
       " \n",
       "         [[False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True],\n",
       "          [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "            True,  True]]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(z, mask_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
