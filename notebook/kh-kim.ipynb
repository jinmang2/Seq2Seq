{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 봤던 seq2seq with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [13, 11, 19],\n",
      "        [25, 22,  2],\n",
      "        [27,  9, 19],\n",
      "        [27,  4,  6],\n",
      "        [12, 13,  8],\n",
      "        [ 5, 12, 13],\n",
      "        [20, 14, 12],\n",
      "        [ 3,  7, 15],\n",
      "        [29, 29, 29]]) torch.Size([10, 3])\n",
      "\n",
      "tensor([[ 0,  0,  0],\n",
      "        [13,  4, 19],\n",
      "        [23, 12, 21],\n",
      "        [29, 14, 28],\n",
      "        [15, 23, 33],\n",
      "        [ 4, 14, 24],\n",
      "        [17, 10, 27],\n",
      "        [20, 24, 18],\n",
      "        [ 6, 26, 16],\n",
      "        [36, 36, 36]]) torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 10\n",
    "BATCH_SIZE = 3\n",
    "INPUT_DIM = 30\n",
    "OUTPUT_DIM = 37\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "ENC_DROPOUT = DEC_DROPOUT = 0.5\n",
    "\n",
    "x = torch.randint(0+1, INPUT_DIM-2, size=(SEQ_LEN, BATCH_SIZE))\n",
    "x[0, :] = 0 \n",
    "x[-1, :] = INPUT_DIM - 1\n",
    "\n",
    "y = torch.randint(0+1, OUTPUT_DIM-2, size=(SEQ_LEN, BATCH_SIZE))\n",
    "y[0, :] = 0\n",
    "y[-1, :] = OUTPUT_DIM - 1\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        H = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = torch.tanh(self.fc(H))\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 emb_dim,\n",
    "                 enc_hid_dim,\n",
    "                 dec_hid_dim,\n",
    "                 dropout,\n",
    "                 attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        return prediction, hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        attn_input = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        energy = torch.tanh(self.attn(attn_input))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        annotation = F.softmax(attention, dim=1)\n",
    "        return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 37])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "김기현님 attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN      = 10\n",
    "BATCH_SIZE   = 3\n",
    "input_size   = INPUT_DIM = 30\n",
    "output_size  = OUTPUT_DIM = 37\n",
    "word_vec_dim = ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "hidden_size  = ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "dropout_p    = ENC_DROPOUT = DEC_DROPOUT = 0.2 # 0.5 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 21, 23, 23, 26,  4, 21, 29,  1,  1],\n",
      "        [ 0,  6, 18,  6,  3,  3, 27, 29,  1,  1],\n",
      "        [ 0, 11, 11, 23, 22, 21, 19,  4, 22, 29]]) torch.Size([3, 10])\n",
      "\n",
      "tensor([[ 0, 30, 25, 15, 11, 33, 17, 36,  1,  1],\n",
      "        [ 0, 17,  2, 21,  6, 10, 36,  1,  1,  1],\n",
      "        [ 0, 27, 34, 28,  3, 34, 18,  5, 14, 36]]) torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "SRC_PAD_IDX = TRG_PAD_IDX = 1\n",
    "MIN_WORDS   = 5\n",
    "\n",
    "src_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "trg_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "if SEQ_LEN - 1 not in src_seq_length:\n",
    "    src_seq_length[-1] = SEQ_LEN - 2\n",
    "if SEQ_LEN - 1 not in trg_seq_length:\n",
    "    trg_seq_length[-1] = SEQ_LEN - 2\n",
    "\n",
    "x = torch.randint(0+2, INPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "x[:, 0] = 0\n",
    "for i, ind in enumerate(src_seq_length):\n",
    "    x[i, ind+1 ] = INPUT_DIM - 1\n",
    "    x[i, ind+2:] = SRC_PAD_IDX\n",
    "\n",
    "y = torch.randint(0+2, OUTPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "y[:, 0] = 0\n",
    "for i, ind in enumerate(trg_seq_length):\n",
    "    y[i, ind+1 ] = OUTPUT_DIM - 1\n",
    "    y[i, ind+2:] = TRG_PAD_IDX\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = y.size(0)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, x_length = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 21, 23, 23, 26,  4, 21, 29,  1,  1],\n",
       "        [ 0,  6, 18,  6,  3,  3, 27, 29,  1,  1],\n",
       "        [ 0, 11, 11, 23, 22, 21, 19,  4, 22, 29]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_src = nn.Embedding(input_size, word_vec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_src_ = emb_src(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 32])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_src_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = nn.LSTM(word_vec_dim,\n",
    "                      hidden_size,\n",
    "                      num_layers=1,\n",
    "                      dropout=0,\n",
    "                      bidirectional=False,\n",
    "                      batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([256, 32]),\n",
       " torch.Size([256, 64]),\n",
       " torch.Size([256]),\n",
       " torch.Size([256])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in list(encoder_rnn.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_src, h_0_tgt = encoder_rnn(emb_src_)\n",
    "h_0_tgt, c_0_tgt = h_0_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 64])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 64]), torch.Size([1, 3, 64]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_tgt.shape, c_0_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_tgt = (h_0_tgt, c_0_tgt)\n",
    "type(h_0_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dec = nn.Embedding(output_size, word_vec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 32])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_tgt = emb_dec(y)\n",
    "emb_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tilde = []\n",
    "h_t_tilde = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = h_0_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
