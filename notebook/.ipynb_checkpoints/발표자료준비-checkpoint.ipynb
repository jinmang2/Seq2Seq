{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "SEQ_LEN     = 15\n",
    "BATCH_SIZE  = 5\n",
    "INPUT_DIM   = 30\n",
    "OUTPUT_DIM  = 37\n",
    "HID_DIM     = 256\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "ENC_DROPOUT = DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 26, 25, 27, 10,  2,  9,  3, 26,  9, 20, 29,  1,  1,  1],\n",
      "        [ 0,  5, 26, 25, 13, 18, 21,  6,  6, 12, 29,  1,  1,  1,  1],\n",
      "        [ 0, 17, 27,  5, 23, 16, 11,  5, 26, 11, 19, 29,  1,  1,  1],\n",
      "        [ 0,  9, 13, 25, 25,  5, 15, 21, 27, 29,  1,  1,  1,  1,  1],\n",
      "        [ 0, 15, 11, 13, 23,  4, 14,  3,  8,  5,  8,  8, 22, 27, 29]]) torch.Size([5, 15])\n",
      "\n",
      "tensor([[ 0, 18,  4,  3,  2,  4, 15, 36,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0,  2, 28, 30, 27,  3, 32,  8, 36,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 22, 26,  9,  7, 21, 22, 36,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 14, 29, 16, 34, 16, 10, 36,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 33,  7, 19, 19, 30, 31, 10, 33, 24,  2,  3,  9, 26, 36]]) torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "SRC_PAD_IDX = TRG_PAD_IDX = 1\n",
    "MIN_WORDS   = 5\n",
    "\n",
    "src_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "trg_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "if SEQ_LEN - 1 not in src_seq_length:\n",
    "    src_seq_length[-1] = SEQ_LEN - 2\n",
    "if SEQ_LEN - 1 not in trg_seq_length:\n",
    "    trg_seq_length[-1] = SEQ_LEN - 2\n",
    "\n",
    "x = torch.randint(0+2, INPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "x[:, 0] = 0\n",
    "for i, ind in enumerate(src_seq_length):\n",
    "    x[i, ind+1 ] = INPUT_DIM - 1\n",
    "    x[i, ind+2:] = SRC_PAD_IDX\n",
    "\n",
    "y = torch.randint(0+2, OUTPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "y[:, 0] = 0\n",
    "for i, ind in enumerate(trg_seq_length):\n",
    "    y[i, ind+1 ] = OUTPUT_DIM - 1\n",
    "    y[i, ind+2:] = TRG_PAD_IDX\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, \n",
    "                          enc_hid_dim, \n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        H = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = torch.tanh(self.fc(H))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, \n",
    "                  ENC_EMB_DIM, \n",
    "                  ENC_HID_DIM, \n",
    "                  DEC_HID_DIM, \n",
    "                  ENC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, hidden = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 15, 128]), torch.Size([5, 64]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.Linear((ENC_HID_DIM * 2) + DEC_HID_DIM, DEC_HID_DIM)\n",
    "v    = nn.Linear(DEC_HID_DIM, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = encoder_outputs.shape[0]\n",
    "src_len    = encoder_outputs.shape[1]\n",
    "batch_size, src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 192])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_input = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "attn_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy = torch.tanh(attn(attn_input))\n",
    "energy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 192])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0681, -0.0243, -0.0442, -0.0483, -0.0822, -0.0971, -0.0957, -0.0324,\n",
       "         -0.0575, -0.0685, -0.0634,  0.0040,  0.0625,  0.0376,  0.0002],\n",
       "        [-0.0816, -0.0831, -0.0451, -0.0357, -0.0238, -0.0653, -0.0959,  0.0309,\n",
       "          0.0625,  0.0946,  0.0811,  0.0908,  0.0891,  0.0769,  0.0400],\n",
       "        [-0.0255, -0.0423,  0.0063,  0.0458,  0.0461,  0.0255,  0.0104, -0.0207,\n",
       "         -0.0012,  0.0412, -0.0437, -0.0008,  0.0510,  0.0682,  0.0592],\n",
       "        [-0.0753, -0.0629, -0.0061, -0.0498, -0.1224, -0.1380, -0.1147, -0.1310,\n",
       "         -0.0648,  0.0158,  0.0538,  0.0454,  0.0311,  0.0213,  0.0067],\n",
       "        [-0.0818, -0.0326, -0.0146, -0.0075,  0.0335,  0.0188,  0.0304,  0.0152,\n",
       "         -0.0397, -0.0828, -0.0442, -0.0273, -0.0453, -0.0897, -0.0477]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = v(energy).squeeze(2)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0647, 0.0676, 0.0662, 0.0659, 0.0637, 0.0628, 0.0629, 0.0670, 0.0653,\n",
       "         0.0646, 0.0650, 0.0695, 0.0737, 0.0719, 0.0692],\n",
       "        [0.0607, 0.0607, 0.0630, 0.0636, 0.0644, 0.0617, 0.0599, 0.0680, 0.0702,\n",
       "         0.0724, 0.0715, 0.0722, 0.0720, 0.0712, 0.0686],\n",
       "        [0.0640, 0.0629, 0.0661, 0.0687, 0.0688, 0.0674, 0.0663, 0.0643, 0.0656,\n",
       "         0.0684, 0.0629, 0.0656, 0.0691, 0.0703, 0.0697],\n",
       "        [0.0642, 0.0650, 0.0688, 0.0658, 0.0612, 0.0603, 0.0617, 0.0607, 0.0649,\n",
       "         0.0703, 0.0730, 0.0724, 0.0714, 0.0707, 0.0697],\n",
       "        [0.0631, 0.0663, 0.0675, 0.0680, 0.0708, 0.0698, 0.0706, 0.0695, 0.0658,\n",
       "         0.0630, 0.0655, 0.0666, 0.0655, 0.0626, 0.0653]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = F.softmax(attention, dim=1)\n",
    "annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 15])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = annotation.unsqueeze(1)\n",
    "annotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = torch.bmm(annotation, encoder_outputs)\n",
    "weighted.shape\n",
    "# (b, n, m) X (b, m, p) ==>> (b, n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_emb = nn.Embedding(OUTPUT_DIM, DEC_EMB_DIM)\n",
    "embedded = dec_emb(y[:, 0].unsqueeze(1))\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 160])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "rnn_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additive Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 26, 25, 27, 10,  2,  9,  3, 26,  9, 20, 29,  1,  1,  1],\n",
       "        [ 0,  5, 26, 25, 13, 18, 21,  6,  6, 12, 29,  1,  1,  1,  1],\n",
       "        [ 0, 17, 27,  5, 23, 16, 11,  5, 26, 11, 19, 29,  1,  1,  1],\n",
       "        [ 0,  9, 13, 25, 25,  5, 15, 21, 27, 29,  1,  1,  1,  1,  1],\n",
       "        [ 0, 15, 11, 13, 23,  4, 14,  3,  8,  5,  8,  8, 22, 27, 29]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lengths = (x != 1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 11, 12, 10, 15])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lengths, ind = torch.sort(x_lengths, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = x.index_select(0, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rev_ind = torch.sort(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_x = sorted_x.index_select(0, rev_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "SEQ_LEN     = 15\n",
    "BATCH_SIZE  = 5\n",
    "INPUT_DIM   = 30\n",
    "OUTPUT_DIM  = 37\n",
    "HID_DIM     = 256\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "ENC_DROPOUT = DEC_DROPOUT = 0.1\n",
    "\n",
    "r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(INPUT_DIM,  # vocab_size\n",
    "                     ENC_EMB_DIM # embedding_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(ENC_EMB_DIM,\n",
    "              ENC_HID_DIM, # hidden_size\n",
    "              num_layers=1,\n",
    "              batch_first=True,\n",
    "              bidirectional=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.Linear(2 * ENC_HID_DIM, # num_directions*hidden_size\n",
    "                 DEC_HID_DIM,     # attention_dimension\n",
    "                 bias=False\n",
    "                )\n",
    "attn2 = nn.Linear(DEC_HID_DIM,    # attention_dimension\n",
    "                  r,              # keywords\n",
    "                                  # (different parts to be expected\n",
    "                                  #  from the sentence)\n",
    "                  bias=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = nn.Tanh()\n",
    "sigmoid = nn.Sigmoid()\n",
    "attn_dist = nn.Softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Sequential(\n",
    "    nn.Linear(r * ENC_HID_DIM * 2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 2), # fc의 hidden_size, output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = embed(sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = Variable(\n",
    "    torch.zeros(1*2, batch_size, ENC_HID_DIM)\n",
    ")\n",
    "cell = Variable(\n",
    "    torch.zeros(1*2, batch_size, ENC_HID_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.6832,  1.8163, -0.6768,  ...,  1.8970,  0.4989,  0.9598],\n",
       "        [ 0.6832,  1.8163, -0.6768,  ...,  1.8970,  0.4989,  0.9598],\n",
       "        [ 0.6832,  1.8163, -0.6768,  ...,  1.8970,  0.4989,  0.9598],\n",
       "        ...,\n",
       "        [-1.0283, -2.8824,  0.2092,  ..., -0.9529, -1.0811, -0.3010],\n",
       "        [ 0.7270,  0.1353,  0.4488,  ...,  0.0234,  0.9150,  1.1722],\n",
       "        [-0.0140,  0.8899,  0.0727,  ...,  0.1250,  1.0330, -0.0721]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 3, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed = pack_padded_sequence(embedded, \n",
    "                              sorted_lengths.tolist(), \n",
    "                              batch_first=True)\n",
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = rnn(packed, (hidden, cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0149, -0.0333, -0.0658,  ...,  0.1088,  0.1697, -0.1746],\n",
       "        [ 0.0149, -0.0333, -0.0658,  ...,  0.0406,  0.2472, -0.0383],\n",
       "        [ 0.0149, -0.0333, -0.0658,  ...,  0.0265,  0.1049, -0.0137],\n",
       "        ...,\n",
       "        [ 0.1855,  0.1605,  0.1007,  ...,  0.0813, -0.1200, -0.0586],\n",
       "        [ 0.0753,  0.1276,  0.0922,  ...,  0.0783, -0.0189, -0.0041],\n",
       "        [ 0.0692, -0.0353,  0.0728,  ...,  0.0188,  0.0904, -0.0005]],\n",
       "       grad_fn=<CatBackward>), batch_sizes=tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 3, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 64]), torch.Size([2, 5, 64]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, output_lengths = pad_packed_sequence(output, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention\n",
    "tanh_a1 = tanh(attn(output))\n",
    "tanh_a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = attn2(tanh_a1)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 15])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = attn_dist(score.transpose(1, 2)) # softmax\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = A.bmm(output)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Penalization\n",
    "eye = Variable(torch.eye(A.size(1)).expand(A.size(0), r, r))\n",
    "eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = torch.bmm(A, A.transpose(1, 2)) - eye\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6865, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_P = ((P**2).sum(1).sum(1) + 1e-10) ** 0.5\n",
    "loss_P = torch.sum(loss_P) / A.size(0)\n",
    "loss_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output\n",
    "M.view(M.size(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1024, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0646, 0.0821],\n",
       "        [0.0565, 0.0867],\n",
       "        [0.0421, 0.0829],\n",
       "        [0.0607, 0.0734],\n",
       "        [0.0531, 0.0873]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(M.view(M.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiplicate attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN      = 10\n",
    "BATCH_SIZE   = 3\n",
    "input_size   = INPUT_DIM = 30\n",
    "output_size  = OUTPUT_DIM = 37\n",
    "word_vec_dim = ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "hidden_size  = ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "dropout_p    = ENC_DROPOUT = DEC_DROPOUT = 0.2 # 0.5 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  7,  6, 18, 27,  2, 29,  1,  1,  1],\n",
      "        [ 0, 16, 27, 19,  9, 15, 21, 24,  6, 29],\n",
      "        [ 0, 16, 27, 21,  6,  2, 11,  2, 10, 29]]) torch.Size([3, 10])\n",
      "\n",
      "tensor([[ 0,  7, 21, 11, 28, 15, 27, 32, 14, 36],\n",
      "        [ 0,  2, 13, 20, 24,  7, 36,  1,  1,  1],\n",
      "        [ 0, 12, 19, 11, 14, 29, 27, 12, 23, 36]]) torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "SRC_PAD_IDX = TRG_PAD_IDX = 1\n",
    "MIN_WORDS   = 5\n",
    "\n",
    "src_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "trg_seq_length = torch.randint(MIN_WORDS, SEQ_LEN-1, (BATCH_SIZE,))\n",
    "if SEQ_LEN - 1 not in src_seq_length:\n",
    "    src_seq_length[-1] = SEQ_LEN - 2\n",
    "if SEQ_LEN - 1 not in trg_seq_length:\n",
    "    trg_seq_length[-1] = SEQ_LEN - 2\n",
    "\n",
    "x = torch.randint(0+2, INPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "x[:, 0] = 0\n",
    "for i, ind in enumerate(src_seq_length):\n",
    "    x[i, ind+1 ] = INPUT_DIM - 1\n",
    "    x[i, ind+2:] = SRC_PAD_IDX\n",
    "\n",
    "y = torch.randint(0+2, OUTPUT_DIM-2, size=(BATCH_SIZE, SEQ_LEN))\n",
    "y[:, 0] = 0\n",
    "for i, ind in enumerate(trg_seq_length):\n",
    "    y[i, ind+1 ] = OUTPUT_DIM - 1\n",
    "    y[i, ind+2:] = TRG_PAD_IDX\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = y.size(0)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, x_length = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_src = nn.Embedding(input_size, word_vec_dim)\n",
    "encoder_rnn = nn.LSTM(word_vec_dim,\n",
    "                      hidden_size,\n",
    "                      num_layers=1,\n",
    "                      dropout=0,\n",
    "                      bidirectional=False,\n",
    "                      batch_first=True)\n",
    "emb_dec = nn.Embedding(output_size, word_vec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_src_ = emb_src(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 64]), torch.Size([1, 3, 64]), torch.Size([1, 3, 64]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_src, h_0_tgt = encoder_rnn(emb_src_)\n",
    "h_0_tgt, c_0_tgt = h_0_tgt\n",
    "h_src.shape, h_0_tgt.shape, c_0_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0_tgt = (h_0_tgt, c_0_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 32])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_tgt_ = emb_dec(y)\n",
    "emb_tgt_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tilde = []\n",
    "h_t_tilde = None\n",
    "decoder_hidden = h_0_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_t = emb_tgt_[:, 0, :].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 32])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_rnn = nn.LSTM(word_vec_dim + hidden_size,\n",
    "                  hidden_size,\n",
    "                  num_layers=1,\n",
    "                  dropout=0.0,\n",
    "                  bidirectional=False,\n",
    "                  batch_first=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t_l_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_l_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 96])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_feeding_trick\n",
    "input_ft = torch.cat([emb_t, h_t_l_tilde], dim=-1)\n",
    "input_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden = dec_rnn(input_ft, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "softmax = nn.Softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = linear(decoder_output.squeeze()).unsqueeze(-1)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.bmm(h_src, query).squeeze(-1)\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "SRC_SEQ_LEN  = 15\n",
    "TRG_SEQ_LEN  = 18\n",
    "BATCH_SIZE   = 5\n",
    "INPUT_DIM    = 30\n",
    "OUTPUT_DIM   = 37\n",
    "HID_DIM      = 256\n",
    "N_SPLITS     = 8\n",
    "N_ENC_BLOCKS = N_DEC_BLOCKS = 6\n",
    "DROPOUT_P    = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 25, 18, 21,  8,  5,  3, 21, 17, 29,  1,  1,  1,  1,  1],\n",
      "        [ 0, 12, 19, 17, 19, 16, 16, 15, 23, 23, 23, 17, 29,  1,  1],\n",
      "        [ 0,  2, 24,  8, 10,  7, 29,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 10,  5, 10, 21, 13, 19, 17,  3, 10,  7,  5,  7, 29,  1],\n",
      "        [ 0, 17, 14,  7,  8, 13, 24,  3,  5, 13, 16,  3, 12, 21, 29]]) torch.Size([5, 15])\n",
      "\n",
      "tensor([[ 0, 10, 29,  5, 19, 33, 36,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [ 0, 27, 33, 27, 23,  3, 23, 11, 21,  6, 25, 33, 12,  7, 33,  7, 36,  1],\n",
      "        [ 0,  4, 28,  3,  2, 12,  2, 10, 11, 18, 26, 27,  2, 36,  1,  1,  1,  1],\n",
      "        [ 0, 27, 14,  5, 13, 34, 34,  6, 19, 30, 22,  4, 15,  2, 18, 36,  1,  1],\n",
      "        [ 0, 25,  6, 17, 15, 28,  3,  8, 29, 27, 25, 33, 10, 18, 13, 19, 15, 36]]) torch.Size([5, 18])\n"
     ]
    }
   ],
   "source": [
    "SRC_PAD_IDX = TRG_PAD_IDX = 1\n",
    "MIN_WORDS   = 5\n",
    "\n",
    "src_seq_length = torch.randint(MIN_WORDS, SRC_SEQ_LEN-1, (BATCH_SIZE,))\n",
    "trg_seq_length = torch.randint(MIN_WORDS, TRG_SEQ_LEN-1, (BATCH_SIZE,))\n",
    "if SRC_SEQ_LEN - 1 not in src_seq_length:\n",
    "    src_seq_length[-1] = SRC_SEQ_LEN - 2\n",
    "if TRG_SEQ_LEN - 1 not in trg_seq_length:\n",
    "    trg_seq_length[-1] = TRG_SEQ_LEN - 2\n",
    "\n",
    "x = torch.randint(0+2, INPUT_DIM-2, size=(BATCH_SIZE, SRC_SEQ_LEN))\n",
    "x[:, 0] = 0\n",
    "for i, ind in enumerate(src_seq_length):\n",
    "    x[i, ind+1 ] = INPUT_DIM - 1\n",
    "    x[i, ind+2:] = SRC_PAD_IDX\n",
    "\n",
    "y = torch.randint(0+2, OUTPUT_DIM-2, size=(BATCH_SIZE, TRG_SEQ_LEN))\n",
    "y[:, 0] = 0\n",
    "for i, ind in enumerate(trg_seq_length):\n",
    "    y[i, ind+1 ] = OUTPUT_DIM - 1\n",
    "    y[i, ind+2:] = TRG_PAD_IDX\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc     = nn.Embedding(INPUT_DIM, HID_DIM)\n",
    "emb_dec     = nn.Embedding(OUTPUT_DIM, HID_DIM)\n",
    "emb_dropout = nn.Dropout(DROPOUT_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 15]), torch.Size([5, 18]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lengths = (x != 1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _generate_mask\n",
    "mask = []\n",
    "max_length = max(x_lengths)\n",
    "for l in x_lengths:\n",
    "    if max_length - l > 0:\n",
    "        mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                            x.new_ones(1, (max_length - l))],\n",
    "                           dim=-1)]\n",
    "    else:\n",
    "        mask += [x.new_ones(1, l).zero_()]\n",
    "mask = torch.cat(mask, dim=0).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 25, 18, 21,  8,  5,  3, 21, 17, 29,  1,  1,  1,  1,  1],\n",
       "        [ 0, 12, 19, 17, 19, 16, 16, 15, 23, 23, 23, 17, 29,  1,  1],\n",
       "        [ 0,  2, 24,  8, 10,  7, 29,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "        [ 0, 10,  5, 10, 21, 13, 19, 17,  3, 10,  7,  5,  7, 29,  1],\n",
       "        [ 0, 17, 14,  7,  8, 13, 24,  3,  5, 13, 16,  3, 12, 21, 29]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x == 1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_enc = mask.unsqueeze(1).expand(\n",
    "    mask.size(0), x.size(1), mask.size(-1))\n",
    "mask_dec = mask.unsqueeze(1).expand(\n",
    "    mask.size(0), y.size(1), mask.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 15, 15]), torch.Size([5, 18, 15]))"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_enc.shape, mask_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 256])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_x = emb_enc(x)\n",
    "embedded_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape is [5, 15, 256]\n",
      "length is 15 and hidden_size is 256\n",
      "enc is tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "position is tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14.])\n",
      "pos div term is tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# _positional_encoding\n",
    "print(f\"x.shape is {list(embedded_x.shape)}\")\n",
    "length, hidden_size = embedded_x.size(1), embedded_x.size(-1)\n",
    "print(f\"length is {length} and hidden_size is {hidden_size}\")\n",
    "enc = embedded_x.new_zeros(embedded_x.shape[1:])\n",
    "print(f\"enc is {enc}\")\n",
    "init_pos = 0\n",
    "pos = init_pos + torch.arange(0, length).unsqueeze(-1)\n",
    "dim = (10000. ** (torch.arange(0, hidden_size//2).div(hidden_size))).unsqueeze(0)\n",
    "assert enc[:, 0::2].size() == (pos / dim).size()\n",
    "assert enc[:, 1::2].size() == (pos / dim).size()\n",
    "pos = pos.float()\n",
    "dim = dim.float()\n",
    "print(f\"position is {pos.flatten()}\")\n",
    "print(f\"pos div term is {dim}\")\n",
    "embedded_x = embedded_x + enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = emb_dropout(embedded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 256])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderBlock\n",
    "# input is z, mask_enc\n",
    "\n",
    "# First Block\n",
    "# MultiHead() -> Dropout() -> Residual Connection -> Seq(fc) -> Dropout() -> Residual Connection\n",
    "# Sublayer's form is LayerNorm(x + sublayer(x))\n",
    "# Seq(fc); Linear -> ReLU(or Leaky-ReLU) -> Linear\n",
    "\n",
    "## MultiHead Attention!!\n",
    "Q_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "K_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "V_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "linear   = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "Q = K = V = z # Encoder는 모두 같음\n",
    "              # (Batch_size, Seg_len, Hid_dim)\n",
    "mask = mask\n",
    "\n",
    "QWs = Q_linear(Q).split(hidden_size // n_splits, dim=-1) # (bsz, seqlen, h/8) * 8\n",
    "KWs = K_linear(K).split(hidden_size // n_splits, dim=-1) # (bsz, seqlen, h/8) * 8\n",
    "VWs = V_linear(V).split(hidden_size // n_splits, dim=-1) # (bsz, seqlen, h/8) * 8\n",
    "\n",
    "QWs = torch.cat(QWs, dim=0) # (bsz*8, seqlen, h/8)\n",
    "KWs = torch.cat(KWs, dim=0) # (bsz*8, seqlen, h/8)\n",
    "VWs = torch.cat(VWs, dim=0) # (bsz*8, seqlen, h/8)\n",
    "\n",
    "mask = torch.cat([mask_enc for _ in range(n_splits)], dim=0) # (bsz*8, m, n)\n",
    "                                                             # in this case, m==n\n",
    "### attention\n",
    "dk = hidden_size // n_splits\n",
    "w = torch.bmm(QWs, KWs.transpose(1, 2)) # reference\n",
    "                                    # (bsz*n_splits, m, n)\n",
    "assert w.size() == mask.size()\n",
    "w.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "w = nn.Softmax(dim=-1)(w / (dk ** 0.5))\n",
    "c = torch.bmm(w, VWs) # (bsz*n_splits, m, hidden_size/n_splits)\n",
    "\n",
    "c = c.split(Q.size(0), dim=0)    # (bsz, m, h/8) * 8\n",
    "c = linear(torch.cat(c, dim=-1)) # (bsz, m, h) -> (bsz, m, h)\n",
    "\n",
    "# Prepare layers\n",
    "attn_dropout = nn.Dropout(dropout_p)\n",
    "attn_norm    = nn.LayerNorm(hidden_size)\n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(hidden_size, hidden_size*4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size*4, hidden_size),\n",
    ")\n",
    "fc_dropout = nn.Dropout(dropout_p)\n",
    "fc_norm    = nn.LayerNorm(hidden_size)\n",
    "\n",
    "z = attn_norm(z + attn_dropout(c))\n",
    "z = fc_norm(z + fc_dropout(fc(z))) # (bsz, n, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None, dk=64):\n",
    "        # |Q| = (batch_size, m, hidden_size)\n",
    "        # |K| = |V| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        w = torch.bmm(Q, K.transpose(1, 2))\n",
    "        # |w| = (batch_size, m, n)\n",
    "        if mask is not None:\n",
    "            assert w.size() == mask.size()\n",
    "            w.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "        w = self.softmax(w / (dk**.5))\n",
    "        c = torch.bmm(w, V)\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return c\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_splits):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "        # Note that we don't have to declare each linear layer, separately.\n",
    "        self.Q_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.K_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.V_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.attn = Attention()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # |Q| = (batch_size, m, hidden_size)\n",
    "        # |K| = |V| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "\n",
    "        QWs = self.Q_linear(Q).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        KWs = self.K_linear(K).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        VWs = self.V_linear(V).split(self.hidden_size // self.n_splits, dim=-1)\n",
    "        # |QW_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        # |KW_i| = |VW_i| = (batch_size, n, hidden_size / n_splits)\n",
    "\n",
    "        # By concatenating splited linear transformed results,\n",
    "        # we can remove sequential operations,\n",
    "        # like mini-batch parallel operations.\n",
    "        QWs = torch.cat(QWs, dim=0)\n",
    "        KWs = torch.cat(KWs, dim=0)\n",
    "        VWs = torch.cat(VWs, dim=0)\n",
    "        # |QWs| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "        # |KWs| = |VWs| = (batch_size * n_splits, n, hidden_size / n_splits)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = torch.cat([mask for _ in range(self.n_splits)], dim=0)\n",
    "            # |mask| = (batch_size * n_splits, m, n)\n",
    "\n",
    "        c = self.attn(\n",
    "            QWs, KWs, VWs,\n",
    "            mask=mask,\n",
    "            dk=self.hidden_size // self.n_splits,\n",
    "        )\n",
    "        # |c| = (batch_size * n_splits, m, hidden_size / n_splits)\n",
    "\n",
    "        # We need to restore temporal mini-batchfied multi-head attention results.\n",
    "        c = c.split(Q.size(0), dim=0)\n",
    "        # |c_i| = (batch_size, m, hidden_size / n_splits)\n",
    "        c = self.linear(torch.cat(c, dim=-1))\n",
    "        # |c| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return c\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_splits,\n",
    "                 dropout_p=.1, use_leaky_relu=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHead(hidden_size, n_splits)\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LeakyReLU() if use_leaky_relu else nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # |x| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, n, n)\n",
    "\n",
    "        z = self.attn_norm(x + self.attn_dropout(self.attn(Q=x,\n",
    "                                                           K=x,\n",
    "                                                           V=x,\n",
    "                                                           mask=mask)))\n",
    "        z = self.fc_norm(z + self.fc_dropout(self.fc(z)))\n",
    "        # |z| = (batch_size, n, hidden_size)\n",
    "\n",
    "        return z, mask\n",
    "    \n",
    "class MySequential(nn.Sequential):\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # nn.Sequential class does not provide multiple input arguments and returns.\n",
    "        # Thus, we need to define new class to solve this issue.\n",
    "        # Note that each block has same function interface.\n",
    "\n",
    "        for module in self._modules.values():\n",
    "            x = module(*x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MySequential(\n",
    "            *[EncoderBlock(\n",
    "                hidden_size,\n",
    "                n_splits,\n",
    "                dropout_p,\n",
    "                False,\n",
    "              ) for _ in range(6)],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 256])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = emb_dropout(embedded_x)\n",
    "z, _ = encoder(z, mask_enc)\n",
    "z.shape # (bsz, n, hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape is [5, 18, 256]\n",
      "length is 18 and hidden_size is 256\n",
      "enc is tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "position is tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17.])\n",
      "pos div term is tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "embedded_y = emb_dec(y)\n",
    "# _positional_encoding\n",
    "print(f\"y.shape is {list(embedded_y.shape)}\")\n",
    "length, hidden_size = embedded_y.size(1), embedded_y.size(-1)\n",
    "print(f\"length is {length} and hidden_size is {hidden_size}\")\n",
    "enc = embedded_y.new_zeros(embedded_y.shape[1:])\n",
    "print(f\"enc is {enc}\")\n",
    "init_pos = 0\n",
    "pos = init_pos + torch.arange(0, length).unsqueeze(-1)\n",
    "dim = (10000. ** (torch.arange(0, hidden_size//2).div(hidden_size))).unsqueeze(0)\n",
    "assert enc[:, 0::2].size() == (pos / dim).size()\n",
    "assert enc[:, 1::2].size() == (pos / dim).size()\n",
    "pos = pos.float()\n",
    "dim = dim.float()\n",
    "print(f\"position is {pos.flatten()}\")\n",
    "print(f\"pos div term is {dim}\")\n",
    "embedded_y = embedded_y + enc\n",
    "\n",
    "h = emb_dropout(embedded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderBlock\n",
    "# input is h, z, mask_dec, None(prev)\n",
    "prev = None\n",
    "key_and_value = z\n",
    "\n",
    "masked_attn = MultiHead(hidden_size, n_splits)\n",
    "masked_attn_norm = nn.LayerNorm(hidden_size)\n",
    "masked_attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "attn = MultiHead(hidden_size, n_splits)\n",
    "attn_norm = nn.LayerNorm(hidden_size)\n",
    "attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "# In the case of inference, we don't have to repeat same feed-forward operations,\n",
    "# Thus, we save previous feed-forward results, including intermediate results.\n",
    "if prev is not None:\n",
    "    z = masked_attn_norm(\n",
    "        h + masked_attn_dropout(\n",
    "            masked_attn(h, prev, prev, mask=None)\n",
    "        )\n",
    "    ) # (bsz, 1, hidden_size)\n",
    "else:\n",
    "    # x shape is (bsz, m, hidden_size)\n",
    "    batch_size = h.size(0)\n",
    "    m = h.size(1)\n",
    "    fwd_mask = torch.triu(h.new_ones((m, m)), diagonal=1).bool() # (m, m)\n",
    "    fwd_mask = fwd_mask.unsqueeze(0).expand(batch_size, *fwd_mask.size()) # (bsz, m, m)\n",
    "    z = masked_attn_norm(\n",
    "        h + masked_attn_dropout(\n",
    "            masked_attn(h, h, h, mask=fwd_mask)\n",
    "        )\n",
    "    ) # (bsz, m, hidden_size)\n",
    "    \n",
    "# key_and_value: (bsz, n, hidden_size)\n",
    "# mask: (bsz, m, n)\n",
    "z = attn_norm(z + attn_dropout(attn(Q=z,\n",
    "                                    K=key_and_value,\n",
    "                                    V=key_and_value,\n",
    "                                    mask=mask_dec))) # (bsz, m, hidden_size)\n",
    "z = fc_norm(z + fc_dropout(fc(z))) # (bsz, m, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 18, 256])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_splits,\n",
    "                 dropout_p=.1, use_leaky_relu=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.masked_attn = MultiHead(hidden_size, n_splits)\n",
    "        self.masked_attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.masked_attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.attn = MultiHead(hidden_size, n_splits)\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LeakyReLU() if use_leaky_relu else nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "        )\n",
    "        self.fc_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x, key_and_value, mask, prev):\n",
    "        # In case of inference, we don't have to repeat same feed-forward operations.\n",
    "        # Thus, we save previous feed-forward results, including intermediate results.\n",
    "        if prev is not None:\n",
    "            # |x| = (batch_size, m=1, hidden_size)\n",
    "            # |prev| = (batch_size, m', hidden_size)\n",
    "\n",
    "            z = self.masked_attn_norm(x + self.masked_attn_dropout(\n",
    "                self.masked_attn(x, prev, prev, mask=None)\n",
    "            ))\n",
    "            # |z| = (batch_size, 1, hidden_size)\n",
    "        else:\n",
    "            # |x| = (batch_size, m, hidden_size)\n",
    "            batch_size = x.size(0)\n",
    "            m = x.size(1)\n",
    "\n",
    "            fwd_mask = torch.triu(x.new_ones((m, m)), diagonal=1).bool()\n",
    "            # |fwd_mask| = (m, m)\n",
    "            fwd_mask = fwd_mask.unsqueeze(0).expand(batch_size, *fwd_mask.size())\n",
    "            # |fwd_mask| = (batch_size, m, m)\n",
    "\n",
    "            z = self.masked_attn_norm(x + self.masked_attn_dropout(\n",
    "                self.masked_attn(x, x, x, mask=fwd_mask)\n",
    "            ))\n",
    "            # |z| = (batch_size, m, hidden_size)\n",
    "\n",
    "        # |key_and_value| = (batch_size, n, hidden_size)\n",
    "        # |mask| = (batch_size, m, n)\n",
    "        z = self.attn_norm(z + self.attn_dropout(self.attn(Q=z,\n",
    "                                                           K=key_and_value,\n",
    "                                                           V=key_and_value,\n",
    "                                                           mask=mask)))\n",
    "        # |z| = (batch_size, m, hidden_size)\n",
    "\n",
    "        z = self.fc_norm(z + self.fc_dropout(self.fc(z)))\n",
    "        # |z| = (batch_size, m, hidden_size)\n",
    "\n",
    "        return z, key_and_value, mask, prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape is [5, 18, 256]\n",
      "length is 18 and hidden_size is 256\n",
      "enc is tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "position is tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17.])\n",
      "pos div term is tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z = emb_dropout(embedded_x)\n",
    "z, _ = encoder(z, mask_enc)\n",
    "z.shape # (bsz, n, hid_dim)\n",
    "# --------------------------------------------\n",
    "embedded_y = emb_dec(y)\n",
    "# _positional_encoding\n",
    "print(f\"y.shape is {list(embedded_y.shape)}\")\n",
    "length, hidden_size = embedded_y.size(1), embedded_y.size(-1)\n",
    "print(f\"length is {length} and hidden_size is {hidden_size}\")\n",
    "enc = embedded_y.new_zeros(embedded_y.shape[1:])\n",
    "print(f\"enc is {enc}\")\n",
    "init_pos = 0\n",
    "pos = init_pos + torch.arange(0, length).unsqueeze(-1)\n",
    "dim = (10000. ** (torch.arange(0, hidden_size//2).div(hidden_size))).unsqueeze(0)\n",
    "assert enc[:, 0::2].size() == (pos / dim).size()\n",
    "assert enc[:, 1::2].size() == (pos / dim).size()\n",
    "pos = pos.float()\n",
    "dim = dim.float()\n",
    "print(f\"position is {pos.flatten()}\")\n",
    "print(f\"pos div term is {dim}\")\n",
    "embedded_y = embedded_y + enc\n",
    "\n",
    "h = emb_dropout(embedded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MySequential(\n",
    "    *[DecoderBlock(\n",
    "        hidden_size,\n",
    "        n_splits,\n",
    "        dropout_p,\n",
    "        False,\n",
    "      ) for _ in range(6)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 18, 256])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, _, _, _ = decoder(h, z, mask_dec, None)\n",
    "h.shape # (bsz, m, hid_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature를 어떻게 학습하는가 + inference는 나중시간에..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
