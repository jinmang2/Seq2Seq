{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [27, 27, 13],\n",
      "        [ 7, 16, 15],\n",
      "        [ 9, 13, 26],\n",
      "        [10,  8,  8],\n",
      "        [18,  7, 10],\n",
      "        [11, 21, 12],\n",
      "        [23, 13,  9],\n",
      "        [27, 17,  7],\n",
      "        [29, 29, 29]]) torch.Size([10, 3])\n",
      "\n",
      "tensor([[ 0,  0,  0],\n",
      "        [27, 17, 12],\n",
      "        [16,  1, 34],\n",
      "        [ 8, 31,  3],\n",
      "        [20, 11, 17],\n",
      "        [28, 27, 12],\n",
      "        [ 4, 19, 20],\n",
      "        [14, 13, 34],\n",
      "        [30, 18, 24],\n",
      "        [36, 36, 36]]) torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 10\n",
    "BATCH_SIZE = 3\n",
    "INPUT_DIM = 30\n",
    "OUTPUT_DIM = 37\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = DEC_HID_DIM = 64\n",
    "ENC_DROPOUT = DEC_DROPOUT = 0.5\n",
    "\n",
    "x = torch.randint(0+1, INPUT_DIM-2, size=(SEQ_LEN, BATCH_SIZE))\n",
    "x[0, :] = 0 \n",
    "x[-1, :] = INPUT_DIM - 1\n",
    "\n",
    "y = torch.randint(0+1, OUTPUT_DIM-2, size=(SEQ_LEN, BATCH_SIZE))\n",
    "y[0, :] = 0\n",
    "y[-1, :] = OUTPUT_DIM - 1\n",
    "\n",
    "print(x, x.shape, end='\\n\\n')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        H = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = torch.tanh(self.fc(H))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, \n",
    "                  ENC_EMB_DIM, \n",
    "                  ENC_HID_DIM, \n",
    "                  DEC_HID_DIM, \n",
    "                  ENC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, hidden = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 128]), torch.Size([3, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg = y[0, :]\n",
    "trg = trg.unsqueeze(0)\n",
    "trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(OUTPUT_DIM, DEC_EMB_DIM)\n",
    "rnn       = nn.GRU((ENC_HID_DIM * 2) + DEC_EMB_DIM, DEC_HID_DIM)\n",
    "fc_out    = nn.Linear(\n",
    "    (ENC_HID_DIM * 2) + DEC_HID_DIM + DEC_EMB_DIM, \n",
    "    OUTPUT_DIM)\n",
    "dropout   = nn.Dropout(DEC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = dropout(embedding(trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 128]), torch.Size([3, 64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.Linear((ENC_HID_DIM * 2) + DEC_HID_DIM, DEC_HID_DIM)\n",
    "v    = nn.Linear(DEC_HID_DIM, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = encoder_outputs.shape[1] # 3\n",
    "src_len    = encoder_outputs.shape[0] # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_input = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "attn_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy = torch.tanh(attn(attn_input))\n",
    "energy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0189,  0.0935,  0.0608,  0.2113,  0.1403,  0.1453,  0.0755,  0.1548,\n",
       "          0.1265,  0.0429],\n",
       "        [-0.0150,  0.0411,  0.0435,  0.0629,  0.0530,  0.0198,  0.0844,  0.1897,\n",
       "          0.0761,  0.1103],\n",
       "        [ 0.0310,  0.0026,  0.1806,  0.2035,  0.0789,  0.1099,  0.0044,  0.1412,\n",
       "          0.0234,  0.0902]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = v(energy).squeeze(2)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0914, 0.0985, 0.0953, 0.1108, 0.1032, 0.1037, 0.0967, 0.1047, 0.1018,\n",
       "         0.0937],\n",
       "        [0.0920, 0.0973, 0.0976, 0.0995, 0.0985, 0.0953, 0.1017, 0.1129, 0.1008,\n",
       "         0.1043],\n",
       "        [0.0944, 0.0917, 0.1096, 0.1121, 0.0990, 0.1021, 0.0919, 0.1054, 0.0937,\n",
       "         0.1001]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = F.softmax(attention, dim=1)\n",
    "annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0914, 0.0985, 0.0953, 0.1108, 0.1032, 0.1037, 0.0967, 0.1047, 0.1018,\n",
       "         0.0937],\n",
       "        [0.0920, 0.0973, 0.0976, 0.0995, 0.0985, 0.0953, 0.1017, 0.1129, 0.1008,\n",
       "         0.1043],\n",
       "        [0.0944, 0.0917, 0.1096, 0.1121, 0.0990, 0.1021, 0.0919, 0.1054, 0.0937,\n",
       "         0.1001]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax\n",
    "a = torch.exp(attention) / \\\n",
    "torch.exp(attention).sum(dim=1).unsqueeze(1)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.unsqueeze(1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = torch.bmm(a, encoder_outputs)\n",
    "weighted.shape\n",
    "# (b, n, m) X (b, m, p) ==>> (b, n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = weighted.permute(1, 0, 2)\n",
    "weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 160])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "rnn_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
